{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d93e842-634a-443d-8258-21d525458ef7",
   "metadata": {},
   "source": [
    "# Let's get scraping!\n",
    "We're going to import the libraries we need for scraping the headline of CNR's website. Then, we'll run this on a schedule using GitHub Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f848bee7-0a67-49e1-8b0d-fa652e717514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391fcc4-a147-4ab9-a141-26f24720ba04",
   "metadata": {},
   "source": [
    "Now, let's specify the URL we want to scrape, then use the `requests` library to fetch the page + headers using the `get` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "059c98a5-d194-4bfd-8c48-ee58c83a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rferl.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dee9783-a2fe-4c98-882e-79b818fcc3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rferl_homepage = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f6e54-585d-41fb-9c2c-aace4cc3406d",
   "metadata": {},
   "source": [
    "We will want to scrape the page we retrieved. That's what we're using [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for. Let's load the page into Beautiful Soup for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fabffe8f-6943-4655-b230-4a398e726430",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(rferl_homepage.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbb11-6737-48eb-9e41-139bbec4bcb4",
   "metadata": {},
   "source": [
    "Grab the front page headline by looking for the first appearance of the `h4` HTML tag. If we were grabbing the top headline from multiple services we would probably have to do this differently (maybe via RSS?) since page layouts are different from service to service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88705f2-094a-470f-bf04-9b211dabcea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polish, Baltic Leaders Head To Kyiv To Bolster Zelenskiy, As Putin Vows To Press Military Campaign\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headline = soup.find('h4')\n",
    "print(headline.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9c446-450e-43af-bf05-dbddc0bcdb98",
   "metadata": {},
   "source": [
    "Let's also add the date and time of retrieval. We'll put the headline and date into an array so we can easily add it as a row to a CSV. We're also adding `.text.strip()` to `headline` to retrieve a text only version of the headline (without HTML tags), and stripping it of newline characters.\n",
    "\n",
    "**NOTE** We are using timezone to set UTC as the time on everything, because that is what Github Actions uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c41cd8-57e2-441d-bbfd-fc0603f572ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polish, Baltic Leaders Head To Kyiv To Bolster Zelenskiy, As Putin Vows To Press Military Campaign', '04/13/2022, 10:39']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "current_time = datetime.now(timezone.utc).strftime(\"%m/%d/%Y, %H:%M\") # no pun intended\n",
    "\n",
    "row = [headline.text.strip(), current_time]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f936d02-8b5e-485d-b8b6-cf219e90d27d",
   "metadata": {},
   "source": [
    "Time to write the result to a CSV, and for that we'll need the Python CSV library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd65b586-52d3-4e0a-82f0-006fab4b0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae181c5-60ce-45fc-90ff-ef957ea30829",
   "metadata": {},
   "source": [
    "And we'll also prepare the headers for the CSV file, and the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96735eba-1fc0-4db7-bc8c-f6caac87f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ['headline','datetime']\n",
    "FILENAME = 'cnr_headlines.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbf6ef-a766-40a5-840e-49e70921a2e2",
   "metadata": {},
   "source": [
    "Let's check if the file already exists. If it does, we'll just add a row, if not, then we create the file from scratch and add the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac870edb-50ab-4dec-8cae-5d4aab981ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILENAME):\n",
    "   with open(FILENAME, 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(HEADERS)\n",
    "    writer.writerow(row)\n",
    "else: # else it exists so append without writing the header\n",
    "   with open(FILENAME, 'a', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898ac57-7dd1-4493-b50e-e16c753eb8c2",
   "metadata": {},
   "source": [
    "Done! üëè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
